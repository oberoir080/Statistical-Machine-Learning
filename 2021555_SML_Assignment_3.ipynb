{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:"
      ],
      "metadata": {
        "id": "no8s7530ju-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate random complex numbers\n",
        "a = np.random.rand(10000) + np.random.rand(10000)*1j\n",
        "b = np.random.rand(10000) + np.random.rand(10000)*1j\n",
        "\n",
        "# Generate random arithmetic operations\n",
        "ops = np.random.choice(['+', '-', '*', '/'], 10000)\n",
        "\n",
        "# Perform the arithmetic operations on the complex numbers\n",
        "c = np.zeros(10000, dtype=np.complex_)\n",
        "for i in range(10000):\n",
        "    if ops[i] == '+':\n",
        "        c[i] = a[i] + b[i]\n",
        "    elif ops[i] == '-':\n",
        "        c[i] = a[i] - b[i]\n",
        "    elif ops[i] == '*':\n",
        "        c[i] = a[i] * b[i]\n",
        "    elif ops[i] == '/':\n",
        "        c[i] = a[i] / b[i]\n",
        "\n",
        "enc = OneHotEncoder()\n",
        "ops_onehot = enc.fit_transform(ops.reshape(-1, 1)).toarray()\n",
        "\n",
        "# Create the dataset\n",
        "data = np.column_stack((np.real(a), np.imag(a), np.real(b), np.imag(b), ops_onehot))\n",
        "labels = np.column_stack((np.real(c), np.imag(c)))\n",
        "\n",
        "# Save the dataset \n",
        "np.savetxt('dataset.csv', np.hstack((data, labels)), delimiter=',')\n",
        "\n",
        "# Define the model\n",
        "model = MLPRegressor(hidden_layer_sizes=(16, 16), activation='relu', solver='adam', max_iter=500)\n",
        "\n",
        "\n",
        "model.fit(data, labels)\n",
        "\n",
        "# Generate random test data\n",
        "test_a = np.random.rand(100) + np.random.rand(100)*1j\n",
        "test_b = np.random.rand(100) + np.random.rand(100)*1j\n",
        "test_ops = np.random.choice(['+', '-', '*', '/'], 100)\n",
        "test_c = np.zeros(100, dtype=np.complex_)\n",
        "for i in range(100):\n",
        "    if test_ops[i] == '+':\n",
        "        test_c[i] = test_a[i] + test_b[i]\n",
        "    elif test_ops[i] == '-':\n",
        "        test_c[i] = test_a[i] - test_b[i]\n",
        "    elif test_ops[i] == '*':\n",
        "        test_c[i] = test_a[i] * test_b[i]\n",
        "    elif test_ops[i] == '/':\n",
        "        test_c[i] = test_a[i] / test_b[i]\n",
        "\n",
        "test_ops_onehot = enc.transform(test_ops.reshape(-1, 1)).toarray()\n",
        "test_a_real = np.real(test_a)\n",
        "test_a_imag = np.imag(test_a)\n",
        "test_b_real = np.real(test_b)\n",
        "test_b_imag = np.imag(test_b)\n",
        "test_data = np.column_stack((test_a_real, test_a_imag, test_b_real, test_b_imag, test_ops_onehot))\n",
        "\n",
        "\n",
        "pred = model.predict(test_data)\n",
        "pred_real = pred[:, 0]\n",
        "pred_imag = pred[:, 1]\n",
        "\n",
        "test_mse = mean_squared_error(np.concatenate((test_a_real + test_b_real, test_a_imag + test_b_imag)), np.concatenate((pred_real, pred_imag)))\n",
        "test_r2 = r2_score(np.concatenate((test_a_real + test_b_real, test_a_imag + test_b_imag)), np.concatenate((pred_real, pred_imag)))\n",
        "\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "mse = mean_squared_error(labels, model.predict(data))\n",
        "r2 = r2_score(labels, model.predict(data))\n",
        "print(f\"Performance:\\nMSE: {mse:.4f}, R-squared: {r2:.4f}\")\n",
        "\n",
        "# print(f\"Testing set performance:\\nMSE: {test_mse:.4f}, R-squared: {test_r2:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNA_yXgKpLlG",
        "outputId": "290d01cf-cc86-46fc-bd19-596e8cfbdcfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance:\n",
            "MSE: 0.0464, R-squared: 0.9238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2:"
      ],
      "metadata": {
        "id": "KMjPRhyLCpc5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXdTzeNjBQxc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iris = load_iris()\n",
        "X = iris.data[:, 2:] \n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "ZVD7l5kfDEQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(y):\n",
        "  hist = np.bincount(y)\n",
        "  ps = hist/len(y)\n",
        "  return(-np.sum([p * np.log2(p) for p in ps if p>0]))"
      ],
      "metadata": {
        "id": "_Y_qw9_ZDI_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "  def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "    self.feature = feature\n",
        "    self.threshold = threshold\n",
        "    self.left = left\n",
        "    self.right = right\n",
        "    self.value = value\n",
        "  \n",
        "  def is_leaf_node(self):\n",
        "    return(self.value is not None)"
      ],
      "metadata": {
        "id": "b0S0NQIHDPIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionTree:\n",
        "  def __init__(self, min_samples_split=2, max_depth=100, n_feats=None):\n",
        "    self.min_samples_split = min_samples_split\n",
        "    self.max_depth = max_depth\n",
        "    self.n_feats = n_feats\n",
        "    self.root = None\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    self.n_feats = X.shape[1] if not self.n_feats else min(self.n_feats, X.shape[1])\n",
        "    self.root = self._grow_tree(X, y)\n",
        "\n",
        "  def _grow_tree(self, X, y, depth=0):\n",
        "    n_samples, n_features = X.shape\n",
        "    n_labels = len(np.unique(y))\n",
        "\n",
        "    #stopping criteria\n",
        "    if(depth >= self.max_depth or n_labels == 1 or n_samples < self.min_samples_split):\n",
        "      leaf_value = self._most_common_label(y)\n",
        "      return(Node(value=leaf_value))\n",
        "    \n",
        "    feat_idxs = np.random.choice(n_features, self.n_feats, replace=False)\n",
        "\n",
        "    #greedy search\n",
        "    best_feat, best_thresh = self._best_criteria(X, y, feat_idxs)\n",
        "\n",
        "    left_idxs, right_idxs = self._split(X[:,best_feat],best_thresh)\n",
        "\n",
        "    left = self._grow_tree(X[left_idxs,:], y[left_idxs], depth+1)\n",
        "    right = self._grow_tree(X[right_idxs,:], y[right_idxs], depth+1)\n",
        "    return(Node(best_feat, best_thresh, left, right))\n",
        "\n",
        "  def _best_criteria(self, X, y, feat_idxs):\n",
        "    best_gain = -1\n",
        "    split_idx, split_thresh = None, None\n",
        "    for feat_idx in feat_idxs:\n",
        "      X_column = X[:, feat_idx]\n",
        "      thresholds = np.unique(X_column)\n",
        "      for threshold in thresholds:\n",
        "        gain = self._information_gain(y, X_column, threshold)\n",
        "        if(gain>best_gain):\n",
        "          best_gain = gain\n",
        "          split_idx = feat_idx\n",
        "          split_thresh = threshold\n",
        "    return(split_idx, split_thresh)\n",
        "  \n",
        "  def _information_gain(self, y, X_column, split_threh):\n",
        "    #parent entropy\n",
        "    parent_entropy = entropy(y)\n",
        "\n",
        "    #generate split\n",
        "    left_idxs, right_idxs = self._split(X_column, split_threh)\n",
        "    if(len(left_idxs == 0) or len(right_idxs)==0):\n",
        "      return 0\n",
        "\n",
        "    #weighted avg vhild entropy\n",
        "    n = len(y)\n",
        "    n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "    e_l, e_r = entropy(y[left_idxs]), entropy(y[right_idxs])\n",
        "    child_entropy = (n_l/n)*e_l + (n_r/n)*e_r\n",
        "\n",
        "    #return ig\n",
        "    ig = parent_entropy - child_entropy\n",
        "\n",
        "    return ig\n",
        "\n",
        "  def _split(self, X_column, split_threh):\n",
        "    left_idxs = np.argwhere(X_column <= split_threh).flatten()\n",
        "    right_idxs = np.argwhere(X_column > split_threh).flatten()\n",
        "    return(left_idxs, right_idxs)\n",
        "\n",
        "  def predict(self, X):\n",
        "    #traverse tree\n",
        "    return(np.array([self._traverse_tree(x, self.root) for x in X]))\n",
        "\n",
        "  def _traverse_tree(self, x, node):\n",
        "    if(node.is_leaf_node()):\n",
        "      return(node.value)\n",
        "\n",
        "    if(x[node.feature] <= node.threshold):\n",
        "      return(self._traverse_tree(x, node.left))\n",
        "    return(self._traverse_tree(x, node.right))\n",
        "\n",
        "  def _most_common_label(self, y):\n",
        "    counter = Counter(y)\n",
        "    most_common = counter.most_common(1)[0][0]\n",
        "    return(most_common)"
      ],
      "metadata": {
        "id": "oL5Oq6hJDXSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decTree = DecisionTree(max_depth=8)\n",
        "fit_decTree = decTree.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "NJq1dUhzD1NY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = decTree.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGSgHmuKD5OT",
        "outputId": "e09b908e-034d-4cff-f9d7-66395ac44ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "import graphviz\n",
        "\n",
        "decTree = DecisionTreeClassifier(max_depth=8)\n",
        "decTree.fit(X, y)\n",
        "\n",
        "dot_data = export_graphviz(decTree, out_file=None, \n",
        "                           feature_names=iris.feature_names[2:],\n",
        "                           class_names=iris.target_names,\n",
        "                           rounded=True, filled=True)\n",
        "\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 916
        },
        "id": "C7EREpSei8g5",
        "outputId": "f0925736-217c-4a9e-876a-0a33b9612a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"734pt\" height=\"671pt\"\n viewBox=\"0.00 0.00 734.00 671.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 667)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-667 730,-667 730,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M435,-663C435,-663 304,-663 304,-663 298,-663 292,-657 292,-651 292,-651 292,-592 292,-592 292,-586 298,-580 304,-580 304,-580 435,-580 435,-580 441,-580 447,-586 447,-592 447,-592 447,-651 447,-651 447,-657 441,-663 435,-663\"/>\n<text text-anchor=\"middle\" x=\"369.5\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) &lt;= 0.8</text>\n<text text-anchor=\"middle\" x=\"369.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n<text text-anchor=\"middle\" x=\"369.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n<text text-anchor=\"middle\" x=\"369.5\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n<text text-anchor=\"middle\" x=\"369.5\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M337,-536.5C337,-536.5 244,-536.5 244,-536.5 238,-536.5 232,-530.5 232,-524.5 232,-524.5 232,-480.5 232,-480.5 232,-474.5 238,-468.5 244,-468.5 244,-468.5 337,-468.5 337,-468.5 343,-468.5 349,-474.5 349,-480.5 349,-480.5 349,-524.5 349,-524.5 349,-530.5 343,-536.5 337,-536.5\"/>\n<text text-anchor=\"middle\" x=\"290.5\" y=\"-521.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"290.5\" y=\"-506.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n<text text-anchor=\"middle\" x=\"290.5\" y=\"-491.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n<text text-anchor=\"middle\" x=\"290.5\" y=\"-476.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M342.09,-579.91C334.49,-568.65 326.23,-556.42 318.59,-545.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"321.39,-543 312.89,-536.67 315.59,-546.91 321.39,-543\"/>\n<text text-anchor=\"middle\" x=\"308.14\" y=\"-557.51\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M518,-544C518,-544 379,-544 379,-544 373,-544 367,-538 367,-532 367,-532 367,-473 367,-473 367,-467 373,-461 379,-461 379,-461 518,-461 518,-461 524,-461 530,-467 530,-473 530,-473 530,-532 530,-532 530,-538 524,-544 518,-544\"/>\n<text text-anchor=\"middle\" x=\"448.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) &lt;= 1.75</text>\n<text text-anchor=\"middle\" x=\"448.5\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"middle\" x=\"448.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n<text text-anchor=\"middle\" x=\"448.5\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n<text text-anchor=\"middle\" x=\"448.5\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M396.91,-579.91C402.91,-571.01 409.33,-561.51 415.53,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"418.44,-554.27 421.14,-544.02 412.64,-550.35 418.44,-554.27\"/>\n<text text-anchor=\"middle\" x=\"425.9\" y=\"-564.86\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#4de88e\" stroke=\"black\" d=\"M427.5,-425C427.5,-425 283.5,-425 283.5,-425 277.5,-425 271.5,-419 271.5,-413 271.5,-413 271.5,-354 271.5,-354 271.5,-348 277.5,-342 283.5,-342 283.5,-342 427.5,-342 427.5,-342 433.5,-342 439.5,-348 439.5,-354 439.5,-354 439.5,-413 439.5,-413 439.5,-419 433.5,-425 427.5,-425\"/>\n<text text-anchor=\"middle\" x=\"355.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) &lt;= 4.95</text>\n<text text-anchor=\"middle\" x=\"355.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n<text text-anchor=\"middle\" x=\"355.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n<text text-anchor=\"middle\" x=\"355.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n<text text-anchor=\"middle\" x=\"355.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M416.23,-460.91C409.09,-451.92 401.46,-442.32 394.09,-433.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"396.67,-430.67 387.71,-425.02 391.19,-435.03 396.67,-430.67\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#843de6\" stroke=\"black\" d=\"M613.5,-425C613.5,-425 469.5,-425 469.5,-425 463.5,-425 457.5,-419 457.5,-413 457.5,-413 457.5,-354 457.5,-354 457.5,-348 463.5,-342 469.5,-342 469.5,-342 613.5,-342 613.5,-342 619.5,-342 625.5,-348 625.5,-354 625.5,-354 625.5,-413 625.5,-413 625.5,-419 619.5,-425 613.5,-425\"/>\n<text text-anchor=\"middle\" x=\"541.5\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) &lt;= 4.85</text>\n<text text-anchor=\"middle\" x=\"541.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n<text text-anchor=\"middle\" x=\"541.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n<text text-anchor=\"middle\" x=\"541.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n<text text-anchor=\"middle\" x=\"541.5\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>2&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M480.77,-460.91C487.91,-451.92 495.54,-442.32 502.91,-433.05\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"505.81,-435.03 509.29,-425.02 500.33,-430.67 505.81,-435.03\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#3de684\" stroke=\"black\" d=\"M252,-306C252,-306 113,-306 113,-306 107,-306 101,-300 101,-294 101,-294 101,-235 101,-235 101,-229 107,-223 113,-223 113,-223 252,-223 252,-223 258,-223 264,-229 264,-235 264,-235 264,-294 264,-294 264,-300 258,-306 252,-306\"/>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) &lt;= 1.65</text>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.041</text>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 48</text>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 1]</text>\n<text text-anchor=\"middle\" x=\"182.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M295.48,-341.91C281.12,-332.2 265.69,-321.76 250.98,-311.81\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"252.66,-308.72 242.42,-306.02 248.74,-314.52 252.66,-308.72\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M433,-306C433,-306 294,-306 294,-306 288,-306 282,-300 282,-294 282,-294 282,-235 282,-235 282,-229 288,-223 294,-223 294,-223 433,-223 433,-223 439,-223 445,-229 445,-235 445,-235 445,-294 445,-294 445,-300 439,-306 433,-306\"/>\n<text text-anchor=\"middle\" x=\"363.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) &lt;= 1.55</text>\n<text text-anchor=\"middle\" x=\"363.5\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"middle\" x=\"363.5\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"middle\" x=\"363.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"middle\" x=\"363.5\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>3&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M358.28,-341.91C358.85,-333.56 359.45,-324.67 360.05,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"363.54,-316.24 360.73,-306.02 356.56,-315.76 363.54,-316.24\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-179.5C109,-179.5 12,-179.5 12,-179.5 6,-179.5 0,-173.5 0,-167.5 0,-167.5 0,-123.5 0,-123.5 0,-117.5 6,-111.5 12,-111.5 12,-111.5 109,-111.5 109,-111.5 115,-111.5 121,-117.5 121,-123.5 121,-123.5 121,-167.5 121,-167.5 121,-173.5 115,-179.5 109,-179.5\"/>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 47</text>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 0]</text>\n<text text-anchor=\"middle\" x=\"60.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 4&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>4&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M140.17,-222.91C127.98,-211.21 114.68,-198.46 102.5,-186.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"104.72,-184.06 95.08,-179.67 99.88,-189.11 104.72,-184.06\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-179.5C240,-179.5 151,-179.5 151,-179.5 145,-179.5 139,-173.5 139,-167.5 139,-167.5 139,-123.5 139,-123.5 139,-117.5 145,-111.5 151,-111.5 151,-111.5 240,-111.5 240,-111.5 246,-111.5 252,-117.5 252,-123.5 252,-123.5 252,-167.5 252,-167.5 252,-173.5 246,-179.5 240,-179.5\"/>\n<text text-anchor=\"middle\" x=\"195.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"195.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"middle\" x=\"195.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"middle\" x=\"195.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 4&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>4&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M187.01,-222.91C188.2,-212.2 189.49,-200.62 190.69,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"194.19,-189.99 191.81,-179.67 187.23,-189.22 194.19,-189.99\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M396,-179.5C396,-179.5 307,-179.5 307,-179.5 301,-179.5 295,-173.5 295,-167.5 295,-167.5 295,-123.5 295,-123.5 295,-117.5 301,-111.5 307,-111.5 307,-111.5 396,-111.5 396,-111.5 402,-111.5 408,-117.5 408,-123.5 408,-123.5 408,-167.5 408,-167.5 408,-173.5 402,-179.5 396,-179.5\"/>\n<text text-anchor=\"middle\" x=\"351.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"351.5\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"middle\" x=\"351.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"middle\" x=\"351.5\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M359.34,-222.91C358.24,-212.2 357.05,-200.62 355.94,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"359.4,-189.26 354.9,-179.67 352.44,-189.97 359.4,-189.26\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M582.5,-187C582.5,-187 438.5,-187 438.5,-187 432.5,-187 426.5,-181 426.5,-175 426.5,-175 426.5,-116 426.5,-116 426.5,-110 432.5,-104 438.5,-104 438.5,-104 582.5,-104 582.5,-104 588.5,-104 594.5,-110 594.5,-116 594.5,-116 594.5,-175 594.5,-175 594.5,-181 588.5,-187 582.5,-187\"/>\n<text text-anchor=\"middle\" x=\"510.5\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) &lt;= 5.45</text>\n<text text-anchor=\"middle\" x=\"510.5\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"middle\" x=\"510.5\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"middle\" x=\"510.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"middle\" x=\"510.5\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M414.5,-222.91C426.47,-213.38 439.33,-203.15 451.61,-193.37\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"453.94,-195.99 459.59,-187.02 449.59,-190.51 453.94,-195.99\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M491,-68C491,-68 394,-68 394,-68 388,-68 382,-62 382,-56 382,-56 382,-12 382,-12 382,-6 388,0 394,0 394,0 491,0 491,0 497,0 503,-6 503,-12 503,-12 503,-56 503,-56 503,-62 497,-68 491,-68\"/>\n<text text-anchor=\"middle\" x=\"442.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"442.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"middle\" x=\"442.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"middle\" x=\"442.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M485.18,-103.73C479.74,-94.97 473.99,-85.7 468.52,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"471.43,-74.95 463.18,-68.3 465.48,-78.64 471.43,-74.95\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M622,-68C622,-68 533,-68 533,-68 527,-68 521,-62 521,-56 521,-56 521,-12 521,-12 521,-6 527,0 533,0 533,0 622,0 622,0 628,0 634,-6 634,-12 634,-12 634,-56 634,-56 634,-62 628,-68 622,-68\"/>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"middle\" x=\"577.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M535.45,-103.73C540.81,-94.97 546.48,-85.7 551.86,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"554.89,-78.66 557.12,-68.3 548.92,-75 554.89,-78.66\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M579,-298.5C579,-298.5 490,-298.5 490,-298.5 484,-298.5 478,-292.5 478,-286.5 478,-286.5 478,-242.5 478,-242.5 478,-236.5 484,-230.5 490,-230.5 490,-230.5 579,-230.5 579,-230.5 585,-230.5 591,-236.5 591,-242.5 591,-242.5 591,-286.5 591,-286.5 591,-292.5 585,-298.5 579,-298.5\"/>\n<text text-anchor=\"middle\" x=\"534.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"middle\" x=\"534.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"middle\" x=\"534.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"middle\" x=\"534.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>12&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M539.07,-341.91C538.43,-331.2 537.74,-319.62 537.09,-308.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"540.58,-308.44 536.48,-298.67 533.59,-308.86 540.58,-308.44\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M714,-298.5C714,-298.5 621,-298.5 621,-298.5 615,-298.5 609,-292.5 609,-286.5 609,-286.5 609,-242.5 609,-242.5 609,-236.5 615,-230.5 621,-230.5 621,-230.5 714,-230.5 714,-230.5 720,-230.5 726,-236.5 726,-242.5 726,-242.5 726,-286.5 726,-286.5 726,-292.5 720,-298.5 714,-298.5\"/>\n<text text-anchor=\"middle\" x=\"667.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"middle\" x=\"667.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n<text text-anchor=\"middle\" x=\"667.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 43]</text>\n<text text-anchor=\"middle\" x=\"667.5\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 12&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>12&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M585.22,-341.91C597.81,-330.21 611.55,-317.46 624.12,-305.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"626.84,-308.04 631.78,-298.67 622.07,-302.91 626.84,-308.04\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7f6ee7b9d580>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:"
      ],
      "metadata": {
        "id": "KacMito-L7Ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/mt.csv\")\n",
        "df \n",
        "X = df[df.columns[:-1]]\n",
        "X = np.array(X)\n",
        "y1 = df[df.columns[-1]]\n",
        "y1 =np.array(y1)\n",
        "y1 = np.array(y1).reshape(-1, 1)\n",
        "y2 = np.mean(X, axis=1) # labels for regression\n",
        "\n",
        "# Splitting the data into train and validation sets\n",
        "X_train, X_val, y1_train, y1_val, y2_train, y2_val = train_test_split(X, y1, y2, test_size=0.2, random_state=42)\n",
        "\n",
        "# Defining the network architecture\n",
        "n_input = 2 # number of inputs\n",
        "n_hidden1 = 5 # number of hidden units in the first layer\n",
        "n_hidden2 = 4 # number of hidden units in the second layer\n",
        "n_output1 = 3 # number of outputs for classification\n",
        "n_output2 = 1 # number of outputs for regression\n",
        "\n",
        "\n",
        "W1 = np.random.randn(n_input, n_hidden1) # weights from input to hidden1\n",
        "b1 = np.random.randn(n_hidden1) # biases for hidden1 [similarly for the biases and weights below]\n",
        "W2 = np.random.randn(n_hidden1, n_hidden2) \n",
        "b2 = np.random.randn(n_hidden2) \n",
        "W3 = np.random.randn(n_hidden2, n_output1) \n",
        "b3 = np.random.randn(n_output1) \n",
        "W4 = np.random.randn(n_hidden2, n_output2) \n",
        "b4 = np.random.randn(n_output2) \n",
        "\n",
        "def sigmoid(x):\n",
        "  x = np.clip(x, -500, 500) # clipping the input to avoid overflow\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "  return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "def tanh(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "  return 1 - tanh(x)**2\n",
        "\n",
        "# Loss functions\n",
        "def cross_entropy(y_true, y_pred):\n",
        "  y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15) # clipping the output to avoid underflow and log(0)\n",
        "  return -np.sum(y_true * np.log(y_pred))\n",
        "\n",
        "def cross_entropy_prime(y_true, y_pred):\n",
        "  return -y_true / y_pred\n",
        "\n",
        "def mse(y_true, y_pred):\n",
        "  return np.mean((y_true - y_pred)**2)\n",
        "\n",
        "def mse_prime(y_true, y_pred):\n",
        "  return -2 * (y_true - y_pred)\n",
        "\n",
        "\n",
        "lr = 0.01 \n",
        "epochs = 100 # number of epochs\n",
        "\n",
        "\n",
        "alpha = 0.5 # weight for classification loss\n",
        "beta = 0.5 # weight for regression loss\n",
        "\n",
        "# Training the network using stochastic gradient descent\n",
        "for epoch in range(epochs):\n",
        "  # Shuffling the training data\n",
        "  idx = np.random.permutation(len(X_train))\n",
        "  X_train = X_train[idx]\n",
        "  y1_train = y1_train[idx]\n",
        "  y2_train = y2_train[idx]\n",
        "\n",
        "  # Looping over each training example\n",
        "  for i in range(len(X_train)):\n",
        "    # Forward propogation\n",
        "    x = X_train[i][:n_input] # input vector \n",
        "    y_true1 = y1_train[i] # true label for classification\n",
        "    y_true2 = y2_train[i] # true label for regression\n",
        "\n",
        "    z1 = x.dot(W1) + b1 # linear combination for hidden1\n",
        "    a1 = sigmoid(z1) # activation for hidden1\n",
        "\n",
        "    z2 = a1.dot(W2) + b2 \n",
        "    a2 = tanh(z2) \n",
        "\n",
        "    z3 = a2.dot(W3) + b3 # linear combination for output1\n",
        "    a3 = sigmoid(z3) # activation for output1 \n",
        "\n",
        "    z4 = a2.dot(W4) + b4 # linear combination for output2\n",
        "    a4 = z4 # activation for output2 \n",
        "\n",
        "    L1 = cross_entropy(y_true1, a3) # classification loss\n",
        "    L2 = mse(y_true2, a4) # regression loss\n",
        "    L = alpha * L1 + beta * L2 # total loss\n",
        "\n",
        "    # Backward propogation\n",
        "    # Computing the gradients for output2\n",
        "    dL_dz4 = mse_prime(y_true2, a4) # gradient of loss w.r.t. z4\n",
        "    dL_dW4 = a2.reshape(-1, 1).dot(dL_dz4.reshape(1, -1)) # gradient of loss w.r.t. W4\n",
        "    dL_db4 = dL_dz4 # gradient of loss w.r.t. b4\n",
        "    dL_da2 = dL_dz4.dot(W4.T) # gradient of loss w.r.t. a2\n",
        "\n",
        "    # Computing the gradients for output1\n",
        "    dL_dz3 = cross_entropy_prime(y_true1, a3) * sigmoid_prime(z3) \n",
        "    dL_dW3 = a2.reshape(-1,1).dot(dL_dz3.reshape(1,-1)) \n",
        "    dL_db3 = np.sum(dL_dz3, axis=0) \n",
        "    dL_da2 += dL_dz3.dot(W3.T) \n",
        "\n",
        "    # Computing the gradients for hidden2\n",
        "    dL_dz2 = dL_da2 * tanh_prime(z2) # gradient of loss w.r.t. z2\n",
        "    dL_dW2 = a1.reshape(-1,1).dot(dL_dz2.reshape(1,-1)) # gradient of loss w.r.t. W2\n",
        "    dL_db2 = np.sum(dL_dz2, axis=0) # gradient of loss w.r.t. b2\n",
        "    dL_da1 = dL_dz2.dot(W2.T) # gradient of loss w.r.t. a1\n",
        "\n",
        "    # Computing the gradients for hidden1\n",
        "    dL_dz1 = dL_da1 * sigmoid_prime(z1) \n",
        "    dL_dW1 = x.reshape(-1,1).dot(dL_dz1.reshape(1,-1)) \n",
        "    dL_db1 = np.sum(dL_dz1, axis=0) \n",
        "\n",
        "    # Updating the parameters using gradient descent\n",
        "    W1 -= lr * dL_dW1\n",
        "    b1 -= lr * dL_db1\n",
        "    W2 -= lr * dL_dW2\n",
        "    b2 -= lr * dL_db2\n",
        "    W3 -= lr * dL_dW3\n",
        "    b3 -= lr * dL_db3\n",
        "    W4 -= lr * dL_dW4\n",
        "    b4 -= lr * dL_db4\n",
        "\n",
        "  \n",
        "  y_pred1 = [] # predictions for classification\n",
        "  y_pred2 = [] # predictions for regression\n",
        "\n",
        "  for j in range(len(X_val)):\n",
        "    x = X_val[j][:n_input] # input vector \n",
        "    \n",
        "    z1 = x.dot(W1) + b1 # linear combination for hidden1\n",
        "    a1 = sigmoid(z1) # activation for hidden1\n",
        "\n",
        "    z2 = a1.dot(W2) + b2 \n",
        "    a2 = tanh(z2) \n",
        "\n",
        "    z3 = a2.dot(W3) + b3 # linear combination for output1\n",
        "    a3 = sigmoid(z3) # activation for output1 \n",
        "\n",
        "    z4 = a2.dot(W4) + b4 # linear combination for output2\n",
        "    a4 = z4 # activation for output2 \n",
        "\n",
        "    y_pred1.append(np.argmax(a3)) \n",
        "    y_pred2.append(a4[0]) \n",
        "  \n",
        "  acc = accuracy_score(y1_val.argmax(axis=1), y_pred1) \n",
        "  rmse = np.sqrt(mean_squared_error(y2_val, y_pred2)) \n",
        "\n",
        "  print(f\"Epoch {epoch+1}, Loss: {L:.3f}, Accuracy: {acc:.3f}, RMSE: {rmse:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZA8NOa5e0ZC4",
        "outputId": "f817bc5a-98b1-4912-b52c-920713c32e61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.116, Accuracy: 0.333, RMSE: 0.894\n",
            "Epoch 2, Loss: 0.425, Accuracy: 0.333, RMSE: 0.599\n",
            "Epoch 3, Loss: 0.027, Accuracy: 0.467, RMSE: 0.360\n",
            "Epoch 4, Loss: 0.205, Accuracy: 0.433, RMSE: 0.218\n",
            "Epoch 5, Loss: 0.041, Accuracy: 0.600, RMSE: 0.187\n",
            "Epoch 6, Loss: 0.057, Accuracy: 0.600, RMSE: 0.193\n",
            "Epoch 7, Loss: 0.007, Accuracy: 0.667, RMSE: 0.155\n",
            "Epoch 8, Loss: 0.073, Accuracy: 0.667, RMSE: 0.147\n",
            "Epoch 9, Loss: 0.038, Accuracy: 0.667, RMSE: 0.125\n",
            "Epoch 10, Loss: 0.016, Accuracy: 0.733, RMSE: 0.132\n",
            "Epoch 11, Loss: 0.024, Accuracy: 0.733, RMSE: 0.114\n",
            "Epoch 12, Loss: 0.003, Accuracy: 0.733, RMSE: 0.109\n",
            "Epoch 13, Loss: 0.016, Accuracy: 0.733, RMSE: 0.104\n",
            "Epoch 14, Loss: 0.033, Accuracy: 0.733, RMSE: 0.103\n",
            "Epoch 15, Loss: 0.016, Accuracy: 0.733, RMSE: 0.117\n",
            "Epoch 16, Loss: 0.014, Accuracy: 0.733, RMSE: 0.096\n",
            "Epoch 17, Loss: 0.002, Accuracy: 0.733, RMSE: 0.094\n",
            "Epoch 18, Loss: 0.023, Accuracy: 0.733, RMSE: 0.097\n",
            "Epoch 19, Loss: 0.019, Accuracy: 0.733, RMSE: 0.101\n",
            "Epoch 20, Loss: 0.021, Accuracy: 0.733, RMSE: 0.094\n",
            "Epoch 21, Loss: 0.003, Accuracy: 0.733, RMSE: 0.087\n",
            "Epoch 22, Loss: 0.016, Accuracy: 0.733, RMSE: 0.104\n",
            "Epoch 23, Loss: 0.016, Accuracy: 0.833, RMSE: 0.105\n",
            "Epoch 24, Loss: 0.040, Accuracy: 0.700, RMSE: 0.127\n",
            "Epoch 25, Loss: 0.003, Accuracy: 0.733, RMSE: 0.084\n",
            "Epoch 26, Loss: 0.064, Accuracy: 0.700, RMSE: 0.101\n",
            "Epoch 27, Loss: 0.001, Accuracy: 0.733, RMSE: 0.078\n",
            "Epoch 28, Loss: 0.001, Accuracy: 0.733, RMSE: 0.078\n",
            "Epoch 29, Loss: 0.014, Accuracy: 0.733, RMSE: 0.081\n",
            "Epoch 30, Loss: 0.002, Accuracy: 0.767, RMSE: 0.081\n",
            "Epoch 31, Loss: 0.017, Accuracy: 0.733, RMSE: 0.076\n",
            "Epoch 32, Loss: 0.001, Accuracy: 0.767, RMSE: 0.078\n",
            "Epoch 33, Loss: 0.001, Accuracy: 0.733, RMSE: 0.077\n",
            "Epoch 34, Loss: 0.015, Accuracy: 0.733, RMSE: 0.072\n",
            "Epoch 35, Loss: 0.001, Accuracy: 0.800, RMSE: 0.082\n",
            "Epoch 36, Loss: 0.012, Accuracy: 0.767, RMSE: 0.075\n",
            "Epoch 37, Loss: 0.030, Accuracy: 0.700, RMSE: 0.125\n",
            "Epoch 38, Loss: 0.008, Accuracy: 0.767, RMSE: 0.072\n",
            "Epoch 39, Loss: 0.019, Accuracy: 0.733, RMSE: 0.073\n",
            "Epoch 40, Loss: 0.010, Accuracy: 0.767, RMSE: 0.067\n",
            "Epoch 41, Loss: 0.014, Accuracy: 0.733, RMSE: 0.067\n",
            "Epoch 42, Loss: 0.012, Accuracy: 0.767, RMSE: 0.067\n",
            "Epoch 43, Loss: 0.009, Accuracy: 0.733, RMSE: 0.065\n",
            "Epoch 44, Loss: 0.015, Accuracy: 0.700, RMSE: 0.077\n",
            "Epoch 45, Loss: 0.015, Accuracy: 0.767, RMSE: 0.065\n",
            "Epoch 46, Loss: 0.004, Accuracy: 0.700, RMSE: 0.093\n",
            "Epoch 47, Loss: 0.005, Accuracy: 0.767, RMSE: 0.062\n",
            "Epoch 48, Loss: 0.005, Accuracy: 0.767, RMSE: 0.065\n",
            "Epoch 49, Loss: 0.009, Accuracy: 0.800, RMSE: 0.063\n",
            "Epoch 50, Loss: 0.011, Accuracy: 0.767, RMSE: 0.064\n",
            "Epoch 51, Loss: 0.009, Accuracy: 0.800, RMSE: 0.061\n",
            "Epoch 52, Loss: 0.008, Accuracy: 0.800, RMSE: 0.060\n",
            "Epoch 53, Loss: 0.001, Accuracy: 0.767, RMSE: 0.060\n",
            "Epoch 54, Loss: 0.000, Accuracy: 0.767, RMSE: 0.061\n",
            "Epoch 55, Loss: 0.006, Accuracy: 0.767, RMSE: 0.060\n",
            "Epoch 56, Loss: 0.013, Accuracy: 0.800, RMSE: 0.058\n",
            "Epoch 57, Loss: 0.001, Accuracy: 0.800, RMSE: 0.059\n",
            "Epoch 58, Loss: 0.004, Accuracy: 0.800, RMSE: 0.060\n",
            "Epoch 59, Loss: 0.001, Accuracy: 0.700, RMSE: 0.108\n",
            "Epoch 60, Loss: 0.000, Accuracy: 0.700, RMSE: 0.080\n",
            "Epoch 61, Loss: 0.006, Accuracy: 0.867, RMSE: 0.074\n",
            "Epoch 62, Loss: 0.011, Accuracy: 0.767, RMSE: 0.061\n",
            "Epoch 63, Loss: 0.005, Accuracy: 0.800, RMSE: 0.055\n",
            "Epoch 64, Loss: 0.000, Accuracy: 0.800, RMSE: 0.058\n",
            "Epoch 65, Loss: 0.008, Accuracy: 0.800, RMSE: 0.054\n",
            "Epoch 66, Loss: 0.004, Accuracy: 0.733, RMSE: 0.068\n",
            "Epoch 67, Loss: 0.007, Accuracy: 0.733, RMSE: 0.068\n",
            "Epoch 68, Loss: 0.001, Accuracy: 0.833, RMSE: 0.059\n",
            "Epoch 69, Loss: 0.013, Accuracy: 0.700, RMSE: 0.091\n",
            "Epoch 70, Loss: 0.003, Accuracy: 0.800, RMSE: 0.053\n",
            "Epoch 71, Loss: 0.005, Accuracy: 0.800, RMSE: 0.054\n",
            "Epoch 72, Loss: 0.001, Accuracy: 0.833, RMSE: 0.056\n",
            "Epoch 73, Loss: 0.007, Accuracy: 0.800, RMSE: 0.051\n",
            "Epoch 74, Loss: 0.001, Accuracy: 0.833, RMSE: 0.059\n",
            "Epoch 75, Loss: 0.004, Accuracy: 0.833, RMSE: 0.052\n",
            "Epoch 76, Loss: 0.000, Accuracy: 0.800, RMSE: 0.050\n",
            "Epoch 77, Loss: 0.000, Accuracy: 0.833, RMSE: 0.053\n",
            "Epoch 78, Loss: 0.000, Accuracy: 0.833, RMSE: 0.054\n",
            "Epoch 79, Loss: 0.005, Accuracy: 0.867, RMSE: 0.062\n",
            "Epoch 80, Loss: 0.000, Accuracy: 0.800, RMSE: 0.050\n",
            "Epoch 81, Loss: 0.004, Accuracy: 0.833, RMSE: 0.051\n",
            "Epoch 82, Loss: 0.003, Accuracy: 0.833, RMSE: 0.049\n",
            "Epoch 83, Loss: 0.004, Accuracy: 0.833, RMSE: 0.056\n",
            "Epoch 84, Loss: 0.010, Accuracy: 0.767, RMSE: 0.058\n",
            "Epoch 85, Loss: 0.003, Accuracy: 0.833, RMSE: 0.051\n",
            "Epoch 86, Loss: 0.003, Accuracy: 0.833, RMSE: 0.050\n",
            "Epoch 87, Loss: 0.003, Accuracy: 0.833, RMSE: 0.055\n",
            "Epoch 88, Loss: 0.003, Accuracy: 0.833, RMSE: 0.054\n",
            "Epoch 89, Loss: 0.000, Accuracy: 0.733, RMSE: 0.063\n",
            "Epoch 90, Loss: 0.003, Accuracy: 0.833, RMSE: 0.055\n",
            "Epoch 91, Loss: 0.000, Accuracy: 0.800, RMSE: 0.045\n",
            "Epoch 92, Loss: 0.002, Accuracy: 0.800, RMSE: 0.044\n",
            "Epoch 93, Loss: 0.000, Accuracy: 0.833, RMSE: 0.050\n",
            "Epoch 94, Loss: 0.003, Accuracy: 0.833, RMSE: 0.048\n",
            "Epoch 95, Loss: 0.003, Accuracy: 0.800, RMSE: 0.045\n",
            "Epoch 96, Loss: 0.003, Accuracy: 0.833, RMSE: 0.046\n",
            "Epoch 97, Loss: 0.005, Accuracy: 0.833, RMSE: 0.055\n",
            "Epoch 98, Loss: 0.004, Accuracy: 0.833, RMSE: 0.047\n",
            "Epoch 99, Loss: 0.000, Accuracy: 0.833, RMSE: 0.046\n",
            "Epoch 100, Loss: 0.002, Accuracy: 0.867, RMSE: 0.064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "\n",
        "1.   https://www.analyticsvidhya.com/blog/2020/07/neural-networks-from-scratch-in-python-and-r/\n",
        "2.   https://www.freecodecamp.org/news/building-a-neural-network-from-scratch/\n",
        "3. https://www.analyticsvidhya.com/blog/2020/10/all-about-decision-tree-from-scratch-with-python-implementation/\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0nxJIoXp_DJN"
      }
    }
  ]
}